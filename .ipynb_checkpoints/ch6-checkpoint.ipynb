{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning  For Text and Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dl models processing text(seq of words or characters), timeseries or seq of data in general. 2 fund dl algos r recurrent neural\n",
    "networks and ID convnets. \n",
    "Apps:-\n",
    "    document classification and timeseries classification, such as identifying the topi or author\n",
    "    timeseries comparisons such as estimating how closely related two documents or two stock tickers are\n",
    "    se1 2 seq learning such as deconding an English sentence into French\n",
    "    Sentiment analysis such as classifying the sentiment of tweets or movie reviews as +ve or -ve\n",
    "    Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data\n",
    "    \n",
    "E.gs:-\n",
    "    sentiment analysis on the IMDB dataset\n",
    "    temperature forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Working with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dl 4 nlp is patten recognition applied 2 words, sentences & paragraphs in much the same way dat omputer vision is pattern\n",
    "recognition applied 2 pixels. numeric tensors fed into nns as input rather than raw text. Vectorizing text is da process\n",
    "of transforming text in2 numeric tensors. this can b done in multiple ways:-\n",
    "    segment text into words, and transform each word into a vector\n",
    "    segment text into characters and transform each character into a vector\n",
    "    extract n-grams of words or characters, and transform each n-gram into a vector. N-grams r overlapping groups of multiple\n",
    "    consecutive groups or characters.\n",
    "\n",
    "Collectively, the different units in which u can break down text(words, character, or n-grams) are called tokens, and breaking text in2 such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme\n",
    "and then associating numeric vectors with the generated tokens. These vectors, packed into sequence tokens are fed in2 deep nns.There r multiple ways 2 associate a vector with a token. In this section, I will present 2 major ones: one-hot encoding of\n",
    "tokens and token-embedding(usd excl 4 words kald word embedding). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding n-grams & bag of words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word n-grams r grups of N (or fewer) consecutive words that u can extract 4rm a sentence. The same concept may be applied to \n",
    "characters instead of words. \n",
    "\n",
    "Consider da example \" The cat sat on the mat\"  may be decomposed in2 the foloowing set of 2-grams:-\n",
    "    {\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n",
    "    \n",
    "    may also be decomposed into following set of 3-grams:-\n",
    "        {\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\", \"sat on the\",\n",
    "        \"the mat\", \"mat\", \"on the mat\"}\n",
    "        \n",
    "Such above set is called bag-of-2 grams or bad-of-3-grams respectively. The term bag refers 2da fact that ur dealing with a set \n",
    "of tokens rather than a list or sequence: the tokens have no specific order. This family of tokenization is kald bag-of-words.\n",
    "    \n",
    "1d cns and rnns r capable of learning reps 4 groups of wordsncharacters vidout being explicitly told abt da existence of such\n",
    "groups, by luking at continuous word or character sequences. however dey r unavoidable feature-engineering tools when using lightweight,\n",
    "shallow text-processing models such as logistic regression and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.1 One hot-encding of words and characters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "most comon way 2 convert token in2 vector. associates a unique integer index with every word and then turns that integer index i\n",
    "into a binary vector of size N (the size of vocabulary); the vector is all zeros except for the ith entry, which is 1. \n",
    "e.g. 6.1 4 words\n",
    "e.g. 6.2 4 characers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Word-level one-hot encoding(toy-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.' ] # initial data: one entry/sample(here a sample is a sentence\n",
    "                                                                   # bit it cud b an entire document)\n",
    "    \n",
    "token_index = {}     #Builds an index of all tokens in the data\n",
    "for sample in samples:         \n",
    "    for word in sample.split():  # Tokenizes da sample via split method. In real life, strip punctuaton & special characters.\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1  #assigns uniq index 2each uniq word. index 0 nt attributed 2 anything.\n",
    "            \n",
    "    max_length = 10    # vectorizes the samples. only consider da 1st max_length words in each sample.\n",
    "    \n",
    "    results = np.zeros(shape=(len(samples),\n",
    "                              max_length,\n",
    "                              max(token_index.values()) + 1))   # results r stored here\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "            index = token_index.get(word)\n",
    "            results[i, j, index] = 1\n",
    "            \n",
    "\n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Character-level one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable   # All printable ASCII charcters\n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras has buitlin utilites 4 one-hot encoding of text @ word/character levels starting 4rm raw data. should use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Using Keras for word-level one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)     #Creates a tokenizer, configd to take only into account the 1000 most common words\n",
    "tokenizer.fit_on_texts(samples)         # Builds the word index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)    #Turns strings into lists of integer indices\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')  #Cud also directly get 1-hot binary reps.\n",
    "                        # Vectorization modes other than one-hot encoding are supported b y this tokenizer\n",
    "    \n",
    "word_index = tokenizer.word_index     # How u can recover the word index that was computed\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot hashing trick"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "used wen no. of unique tokens is vocublary is 2 lrg 2b handled explicitly. Instead of explicitly assigning an index 2 each word\n",
    "and keeping a reference of these indices in a dictionary, u can hash words into vectors of fixed size. This is typically donw\n",
    "with a very ligthweight hashking function. The main advantage of this method is that it does away with maintaining an eplicit word\n",
    "index, which saves memory and allows online encoding of the data( you can generate token vectors right away, b4 u've seen all\n",
    "of the data). Drawbaq=> susceptible 2 hash collissions i.e. 2 different words may end up with the same hash., and subsquently\n",
    "any ml model luking at these hhashes wont beable to tell the diff btw these words. The likelihood of hash collissions decreses\n",
    "when the dimensionality of the hashing space is much larger than the total no. of uniquetokens beng hashed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Word-Level one-hot encoding with hashing trick (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework' ]\n",
    "\n",
    "dimensionality = 1000    # Stores the words as vectors of size 1000. If u've close 2 1000 words(or more), u'll see many hash \n",
    "                         # collissions, which will decrease the accuracy of this encoding method.\n",
    "max_length= 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality  # Hashes the word into a random index integer btw 0 and 1000\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.2 Using word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "low-dimensional floating-point vectors(dense vectors as opposed to sparse vectors). Unlike the word vectors obtained via one-hot\n",
    "encoding, word embeddings are learned from data. it's common 2c word-embeddings dat r 256-dimensional, 512-dimsensional, 1024\n",
    "dimensional when dealing with very lrg vocabularies. On the other hand, one-hot encodings words generally leads to vectors that\n",
    "r 20,000 dimensional or greatere(capturing a vocabularty of 20000 tokens in this case). so word embeddingsz pack more info into \n",
    "far fewer dimensions/.\n",
    "\n",
    "one-hot word vectors are sparse-highdimensional and hardcored.\n",
    "word embeddings are dense, lower-dimensional and learned from data.\n",
    "\n",
    "2 ways to obtain word embeddings\n",
    "\n",
    "1- learn woprd embeddings hointly with main task u care abt. (such as document classification or sentiment prediction). u start \n",
    "with random vectors and then learn word vectors in the same way u learn the weights of a nn.\n",
    "\n",
    "2. load into your model word embeddings that were precomputed using a different machine-learning task than the one u'r trying \n",
    "to solve. These are called pretrained word embeddings.\n",
    "\n",
    "let's luk at both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning word embeddings with the embedding layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "simplest way 2 associate a dense vector with a word is2 choose the vector at random. The problem with ths approach is that the \n",
    "resulting embedding space has no structure: i.e., the words accurate and exact may end up wth completely different \n",
    "embeddings, even though they r interchangable in most sentences. its difficult for a dnn 2 make sense of such a noisy, unstructured embedding \n",
    "space.\n",
    "2get a bit more abstract, the geometric relationships btw word vectors should reflect the semantic relationships btw these \n",
    "words. word embeddings r meant to map human language into a geometric space. i.e. in a reasonable embedding space, u shud expect \n",
    "synonyms 2b embedded into similar word vectors; and in general u wud expect the geometric distance (such as L2 distance) btw any\n",
    "2 word vectors 2 relate to the semantic distance bt2 the associiated words(words meaning diff things r embedded at points far \n",
    "away 4rm each other, whereas relate words are closer.). In addition to distance, u many want specific directions in the embedding\n",
    "space to be meaninful.\n",
    "e.g., embed 4 words on a 2D plane: cat, dog, wold and tiger. With the vector reps we chose jhere, some semantic relationships\n",
    "btw these words can b encoded as geometric transformations. i.e. \"pet to wild animal\" vector allows us2 go 4rm cat 2 tiger & 4rm\n",
    "dog 2 wolf. \"from canine to feline\" vector allows us2 go 4rm dog 2 cat and wold 2 tiger.\n",
    "real world word embedding spaces, eg';s of meaningful geometric transformations are \"gender\" vectors and \"plural\" vectors. i.e.\n",
    "by addn a femal vector 2 da vector king we get vector queen and addn plural vectoer we obtain kings. word embedding spaces \n",
    "typically feature thousands of such interpretable and potentially useful vectors.\n",
    "its thus reasonable to learn a new embedding space with every new task. backpropagation makes this easy and Keras makes it even easier. Its \n",
    "abt learning the weights of a layer: the Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Instantiating an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000, 64)  #The Embedding Layer takes at least 2 args: the no. of possible tokens(here, 1,000\n",
    "                                       # 1 + maximum word index) and the dimensionality of the embeddings(here, 64)\n",
    "    \n",
    "#The embedding layer is best understodd as a dictionary that maps integer indices (which stand for specific words) 2 dense vectors.\n",
    "#It takes integers as input, it luks up these integers in an internal dictionary, and it returns the associated vectors. it's\n",
    "#effectively a dictionary lookup.\n",
    "\n",
    "# word index => Embedding layer => Corresponding word vector\n",
    "\n",
    "#the embedding layer takes as input a 2D tensor of integers ,of shape( samples, sequence_length), where each entry is a seq of\n",
    "# integers. it can embed sequences of variable lengths: i.e. u cud feed in2 da Embedding Layer in the previous example batches\n",
    "# with shapes (32, 10)(batch of 32 sequences of length 10) or (64, 15)(batch of 64 sequences of length 15). All seqs in a \n",
    "# batch mst have the same length, though(as they have 2b packed in2 a single tensor), so seqs shorter than others shud b\n",
    "# padded with zeros, and seqs dat r longer shud be truncated.\n",
    "\n",
    "# dis layer return a 3D floating point tensor of shape (samples, sequence_length, embedding_dimensionality). Such a 3D tensor\n",
    "# can then b processed by an RNN layer or a 1D convolution layer. \n",
    "\n",
    "# wen u instantiate an EMbedding latyer, its weights (its internal dict of token vectors) are initally random, just as with \n",
    "# any other layer. Dring training, these word vectors are gradually adjusted via backpropagation, structuring the space in2 sth\n",
    "# the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure- a kind of structure\n",
    "# specialized for the specific problem 4 which ur training ur model.\n",
    "\n",
    "# let's apply his idea to the IMDB movie-review sentiment prediction task. 1st. prepare data. restrict move review 2 top 10000\n",
    "# mst common words and cut off the review after only 20 words. THe network will learn 8 dimensiaol embdngs 4 each of the 10000\n",
    "# words, turn the input integer sequences (2D integer tensor) in2 embedded sequences (3D float tensor), flatten the tensor 2\n",
    "# 2D, and train a single Dense layer on top 4 classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6 Loading the IMDB data for use with an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000   # no. of words 2 consider as features\n",
    "maxlen = 20     #  cuts off the text after this no. of words(among the max_features most common words)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)    # loads da data as list of integerz\n",
    "\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)   # turns the list of integers in2 a 2D integer tensor \n",
    "                                                                         # of shape (samples, maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.7 Using an  Embedding Layer and classifier on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))  #Specifies da max input length 2da Embedding layer so u can later flatten\n",
    "                        # da embedded inputs. Aftr da Embedding layer, the activations have shape (samples, maxlen, 8)\n",
    "\n",
    "model.add(Flatten())    # Flattens the 3D tensor of embeddings in2 a 2D tensor of shape(samples, maxlen * 8)\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))       # adds the classifier on top\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val accuracy of 75%. better add recurrnt layers or !D con layers ontop ofda embedded seqs to take into accnt each seq asawhole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#wen little data available, load embedding vectors 4rm a precomputed embedding space which is highly structured and exhibits\n",
    "# useful props-capturing gen aspects of lang struct. \n",
    "# such word embeddings r generally computed using word-occirence statistics (observations abt what words co-occur in \n",
    "# sentences or documents), using a variety of techniques, some involving nns others not. \n",
    "# der r various precomputed dbs of wordembeddings dat can be downloaded nd used in Keras Embedding Layer. \n",
    "#Word2vec\n",
    "#GloVe(Global Vectors for World Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: 4rm raw text to word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model=embedding sentences into a seq of vectors => flatetning tehm => training a dense layer on top    ===using \n",
    "   pretrained word embeddings  ////strtng 4rm scratch by downloading the orgnal text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the IMDB data as raw text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://mng.bz/0tlo.       download the raw IMDB dataset. uncompress it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.8 Processing the labels of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = '/home/remo/Documents/aclImdb/'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    " \n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZING THE DATA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "vectorize text & prepare a training & validation split. pretrained word embeddings useful 4 little training data\n",
    "cases, therefore, restrict da training data 2 1st 200 samples. so u'll learn 2 classify movie reviews after\n",
    "lukin at just 200 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the text of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100     # Cuts off reviews after 100 words\n",
    "training_samples = 200     # Trains on 200 samples\n",
    "validation_samples = 10000    # Validates on 10,000 samples\n",
    "max_words = 10000             # Considers only da top 10,000 words in the dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0]) #Splits da data into a training set and a validation set,bt 1st shuffles da data\n",
    "                                   #coz ur starting with data in which samples r ordered (all -ve 1st, then +ve)\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOWNLOADING THE GLOVE WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "go2 https://nlp.stanford.edu/projects/glove, download precomputed embeddings 4rm 2014 English wikipedia.  \n",
    "822 Mb file called glove.6B.zip, containing 100-D embedding vectors 4 400,000 words (or nonword tokens). Unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = ''\n",
    "\n",
    "embeddings_index= {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build an embedding matric 2b loaded in2 Embedding layer. mst b of matrix of shape (max_words, embedding_dim),\n",
    "where each entry i contains the embedding_dim-dimensional vector 4da word of index i in the ref word index\n",
    "(built during tokenization). index 0 isnt supposed 2 stand 4 any word or token-it's a placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.11 Preparing the Glove word-embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector    #Words not found in the embedding index will all be zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.12 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.13 Loading pretrained word embeddings in2 da Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "freeze the Embedding layer(set its trainable att 2 False), when parts of a model r pretrained (like ur Embedding\n",
    "layer) & parts r randomly initialized(like ur classifier), da pretrained parts shudnt be updated during training,\n",
    "2 avoid 4getting wat dey alredy no. da lrg gradient updates triggered by da randomly init layers wud b disruptive\n",
    "2 da already-learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.14 Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.15 Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label= 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label= 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the model quickly starts overfitting, which is unsurprising given the small no. of training samples, val_acc\n",
    "has high variance 4da same reason, bt it seems 2 reach da high 50s.\n",
    "mileage may vary as so few samples, performance is heavily dependant on exactly which 200 samples u choose- and \n",
    "ur choosing dem at random. if results poor, try choosing a diff random sampl.\n",
    "u can also train da same model without loading the pretrained word embeddings and without freezing the embedding\n",
    "layer. in that case, u'll learn a task specific embedding of the input tokens, which is genreally more powerful\n",
    "dan pretrained word embeddings wen lots of data is available. in dis case, u only 've 200 training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.16 Training the same model vidout pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Val_acc stalls in the low 50s. pretrained word embeddings outperform jointly learned embeddings. if u increase\n",
    "da no. of training samples, dis 'll quickly stop beng da case.\n",
    "evaluate the model on the test data. 1st tokenize the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.17 Tokenizing the data of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "                \n",
    "            else:\n",
    "                labels.append(1)\n",
    "                \n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.18 Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appalicing test_acc of 56%. 2 few training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.4 Wrapping up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn raw text in2 sth a nn cn process\n",
    "\n",
    "# Use da Embedding layer in a Keras model 2 learn task-specific token embeddings\n",
    "\n",
    "# Use pretrained word embeddings 2get an extra boost on small nlp problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Understanding recurrent neural networks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a major characterstic of all nn so far such as densely connected networks and convnets is that they have no memory.\n",
    "each input shown 2 dem is processed independently with no state kept in btw inputs. with such networks in order 2 \n",
    "process a sequence or temporal series of data points u have 2 show da entire sequence 2 da network at once turn\n",
    "it into a single data point. i.e. IMDB eg. an entire movie review was transformed in2 a single lrg vector and \n",
    "processed in 1 go. such networks are called feedforward networkds.\n",
    "\n",
    "in contrast, as ur reading the present senctenxe, ur processing it word by word-or rather, eye saccade by \n",
    "eye saccade- while keeping memories of what came b4; dis gives u a fluid rep of the meaning conveyed by dis \n",
    "sentence. Bio intel processes info incrementally while maintaing an internal model of what it's processing. built 4rm\n",
    "past info and constantly updated as new info comes in.\n",
    "\n",
    "a recurrent neural network(RNN) adopts the same principle. it processes sequences by iterating thru the sequence\n",
    "elements & maintaining a state containing info rel 2 what it has cen so far. RNN is a type of nn with an internal\n",
    "loop. the state of rnn is reset btw processing 2 diff indpendent seqs (such as 2 diff IMDB reviews), considering \n",
    "1 seq a single data point: a single input 2da network. What changes is da this data point is no longer processed \n",
    "in a single step; rather, the network internally loops over sequence elements.\n",
    "\n",
    "to make des notions of loop & state clear, let's implement da 4wrd pass of a toy RNN in Numpy. This RNN takes as \n",
    "input a seq of vectors, which u'll encode as a 2D tensor of size (timesteps, input_features). It lups over\n",
    "timesteps, & @ each timestep, it considers its current state at t and the input at t(of shape (input_features,))\n",
    "and combines dem to obtain the output at t. U'll den set the state 4da next step 2b dis prev output. for the \n",
    "first timestep, the prev output isnt defined; hence there is no current state. so u'll init da state as an all-0\n",
    "vector called the initial state of the network.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.19 Pseudocode RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = 0          # The state at t\n",
    "for input_t in input_sequence:      # iterates over sequence elements\n",
    "    output_t = f(input_t, state_t)\n",
    "    state_t = output_t         # the prev output becomes the state for the next iteration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "u can flesh out the function f: the transformation of the input and state into an output will be parametrized in2\n",
    "an output wilb parametrized by 2 matrices, W & U, and a bias vector. it's similar 2da transformation operated by a \n",
    "densely connected layer in a feedforward network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.20 More detailed pseudocode for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = 0\n",
    "for input_t in input_sequence:\n",
    "    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
    "    state_t = output_t\n",
    "# 2make des notions absolutely unambiguous, let's write a naive Numpy implementation of da 4wd pass of da simple RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.21 Numpy implementation of a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "timesteps = 100       # No. of timesteps in the input sequence\n",
    "input_features = 32   # dimensionality of da input feature space\n",
    "output_features = 64  # dimensionality of the output feature space\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))  # Input data: random noise 4da sake of eg\n",
    "\n",
    "state_t = np.zeros((ouput_features,))    # initial state: an all-zero vector\n",
    "\n",
    "W = np.random.random((output_features, input_features))        # Creates random weight matrices\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features,))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)  #input_t is a vector of shape(input_features)\n",
    "          # combines da input vid da current state(prev output) 2 obtain the current output\n",
    "    successive_outputs.append(output_t)            # stores this output in a list\n",
    "    \n",
    "    state_t = output_t       # Updates the state of the network for the next timestep\n",
    "    \n",
    "    final_output_sequence = np.concatenate(successive_outputs, axis =0)  # final output is a 2D tensor of shape\n",
    "                                                                            (timesteps, output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN is a for loop dat reuses qtts computed during da prev iteratino of the loop. nothing more. many rnns fitting\n",
    "dis definition cud b built, dis e.g. is one of the simplest RNN formulations. RNNs are characterized by der step\n",
    "fn such as the following fn in dis case\n",
    "\n",
    "output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.21 A recurrent layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "\n",
    "#SimpleRNN processes btaches of seqs like all other Keras layers, not a sngle seq as inda Numpy eg. inputs of shape\n",
    "#(batch_size, timesteps, input_features)  rather than (timesteps, input_features)\n",
    "\n",
    "#lik all recurrent layers in Keras, SimpleRNN can b run in2 diff modes: it can run either da full seqs of successiv\n",
    "#outputs 4 each timestep (a 3D tensor of shape (batch_size, timesteps, output_features)) or only da last output 4\n",
    "# each input sequence (a 2D tensor of shape (batch_size, output_features)). des 2 modes r controlled by da\n",
    "# return_sequences constructor arg. let's luk @ an e.g. using SimpleRNN rtrning only output at last timestep:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following e.g rtrns da full state seq.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes useful 2 stack several recurrent layers one afte the other in order 2 increase the representational\n",
    "# power of a netwrok. here, get all of the intermediate layers 2 rtrn full seqs of outputs:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))       # last layer rtrns only da last output\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use such a model on the IMDB movie-review classification problem. 1st preprocess da data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.22 Preparing the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000          # No. of words 2 consider as features\n",
    "maxlen = 500      # cuts off texts after dis many words(among the max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train), 'train_sequences')\n",
    "print(len(input_test), 'test_sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.23 Training the model with Embedding and SimpleRNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size = 128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display da traiing & val loss and acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.24 Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label= 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label= 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a reminder, in ch 3 dis dataset got test_acc of 88% however, dis small RNN got only 85%. considering da fact\n",
    "# dat only considered the 1st 500 wirds rather dan full seqs. hence, the RNN has access to less info dan da \n",
    "# earlier baseline mode.. The remainder of the problem is dat SimpleRNN isnt good at processing long seqs(lyk text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.2 Understanding the LSTM and GRU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM layer = a variation of SimpleRNN layer. it adds a way 2 carry info across many timesteps. imagine a conveyor \n",
    "belt running parallel to tehe seq ur processing. info 4rm da seq can jump onto da conveyor belt at any point, b\n",
    "transported to a later timestep, and jump off intact when u need it. This is essentially wat LSTM does: it saves\n",
    "info 4 later, thus preventing older signals 4rm gradually vanishing during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.25 Pseudocode details of the LSTM architecture(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6a5cf96b4d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mbi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mbf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mk_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mbk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'activation' is not defined"
     ]
    }
   ],
   "source": [
    "output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\n",
    "\n",
    "i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) +bi)\n",
    "f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) +bf)\n",
    "k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) +bk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the new carry state (the next c_t) by combining i_t, f_t, and k_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.26 Pseudocode details of the LSTM architecture(2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_t +1 = i_t * k_t + c_t * f_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just keep in mind what the LSTM cell is meant to do: allow past information 2b reinjected at a later time,\n",
    "# thus fighting the vanishing-gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.3 A concrete LSTM example in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.27 Using the LSTM layers in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history= model.fit(input_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_acc=89%. much better dan SimpleRNN mainly coz LSTM suffers much less 4rm the vanishing-gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.24 Wrapping Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#know\n",
    "\n",
    "# what RNNs are and how they work\n",
    "\n",
    "# what LSTM is and why it works better on long sequences than a naive RNN\n",
    "\n",
    "# How to use Keras RNN layers to process sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Advance use of RNNs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 3 advanced techniques 4 improving da performance and generalization power of rnns. \n",
    "# demonstrate all 3 concepts on a temperature forecasting problem where u've access to a timeseries of data points \n",
    "# cumn 4rm sensors installed on building roof such as temperature, air pressure and humidity which is usd 2 prdct\n",
    "# wat da temp wilbe 24 hours after da last data point. \n",
    "\n",
    "# cover the following techniques.\n",
    "\n",
    "# Recurrent dropout---a specific builtin way to use dropout to fight overfitting in recurrent layers.\n",
    "\n",
    "# Stacking recurrent layers---increasa da reprsenttional power ofda ntwork(atda cost of higher computational loads)\n",
    "\n",
    "#Bidirectional recurrent layers---present same info to rn in diff ways-increasing acc & decreasing 4getting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.1 A temperature forecasting problem"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "14 diff qtts (temp, atm pre, humidity, wind direction etc) recorded every 10min. dataset perfect 2 work with \n",
    "numerical timeseries. build a model input sum data 4rm recent past & predict air temp 24hrs inda future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.28 Inspecting the data of the Jena weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir ='/home/remo/Downloads/jena_climate'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))\n",
    "\n",
    "# dis output a count of 420551 lines of data(each line is a timestep: a record of a date and 14 weather-related\n",
    "# values), as well as the following header:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all 420,551 lines of data into a Numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.29 Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(','[1:])]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the temperature timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot as plt\n",
    "\n",
    "temp = float_data[:, 1] \n",
    "plt.plot(range(len(temp)), temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a more narrow plot of the first 10 days of temperature data. as data is recorded every 10 mins\n",
    "# u get 144 data points/ day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.31 Plotting the first 10 days of the temperature timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1440), temp[:1440])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on this plot u can see daily periodicity, esp evident 4da last 4 days. dis 10-day period 4rm cold winter month.\n",
    "is this timeseries predictable at a daily scale? let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.2 Preparing the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "exact formulation of problem => given data going as far basck as lookback timesteps ( a timestep is 10 minutes)\n",
    "and samples every steps timesteps, can u predict the temperature in delay timesteps. following params values used:\n",
    "    \n",
    "    lookback = 720 --- observations will go back 5 days\n",
    "    steps    = 6   --- obs will b sampled at one data point /hour\n",
    "    delay    = 144 --- targets will be 24 hours in the future\n",
    "    \n",
    " to get started, do 2 things:\n",
    " \n",
    " preprocess the data to format a nn can ingest. data alrdy numerical dont nid vectorization. but each timeseries in the data is on a different scale(e.g. temp btw -20 & 30 etc whereas atm press (mbar) around 1000). normalize\n",
    " each timeseries independently so that they take small values on a similar scale.\n",
    " \n",
    " write a Python generator that takes da current array of float data and yields branches of data 4rm da recent\n",
    " past alongwith a target temperature in the future. as the sammples in the dataset r highly redundant(sample\n",
    " N & sample N+1 will 've most of their timesteps in common), it would be wasteful 2 explicitly allocate\n",
    " every sample. instead generate samples on the fly using orignal data.\n",
    " \n",
    " preprocess the data by subtracting the mean of each timeseries and dividing by the standard deviation. using \n",
    " first 200,000 timesteps as training data hene computing the mean and std only on this fraction of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.32 Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Listing 6.33 shows the data generator. yielding a tuple( sample, targets), where samples is one batch of input\n",
    "data and targets is the corresponding array of target temperatures. takes following args.\n",
    "\n",
    "-data---the original array of floating-point data, which u normalized in listing 6.32.\n",
    "-lookback---how many timesteps back the input data should go.\n",
    "-delay---how many timesteps in the future the target shud be.\n",
    "-min_index and max_index---indices in the data array dat delimit which timesteps 2 draw 4rm. useful 4 keeping a \n",
    "segment of the data for validation and another for testing.\n",
    "-shuffle---whether to shuffle the samples or draw them in chronological order.\n",
    "-batch-size---the no. of samples per batch.\n",
    "step---the period in timesteps at which u sample data. set it to 6, 2 draw one data point every hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator (data, lookback, delay, min_index, max_index,\n",
    "               shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else: \n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "            \n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let's use da abstract generator fn to instantiate 3 generators: one 4 training, one 4 validation, and one 4 testing\n",
    "each will luk at different temporal segments of the orignial data: the training genrtor luks at first 200,000\n",
    "timesteps, val gen luks at following 100000 and test gen luks @ remainder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.34 Preparing the training, validation and test generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen  =  generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=200001,\n",
    "                      max_index=300000,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=300001,\n",
    "                      max_index=None,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "\n",
    "val_steps = (300000 - 200001 - lookback) # How many steps 2 draw 4rm val_gen in order 2 c da entire validation set\n",
    "\n",
    "test_steps = (len(float_data) - 300001 - lookback) # steps 2draw 4rm test_gen in order2 c da entire test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.3 A common-sense, non-machine-learning baseline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the temperature timeseries can safely be assumed 2b continuous(i.e. the temp 2morrow likely 2b close 2 today) as\n",
    "well as periodical with a daily period. common-sense approach is2 always predict that da temp 24 hours 4rm now eqal\n",
    "2 temp now. evaluate dis approach using the mean absolute error (MAE) metric:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.mean(np.abs(preds - targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.3.5 Computing the common-sense baseline MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "    \n",
    "evaluate_naive_method()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "this yields an MAE of 0.29. as the temp data has been normalized 2b centered on 0 and 've  a std of 1, this no. \n",
    "isn't immediately interpretable. it translates to an average absolute error of 0.29 x temperature_std degrees\n",
    "Celsius: 2.57 degree C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.36 Converting the MAE back to a Celsius error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celsius_mae = 0.29 * std[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "that's a fairly large avg abs err. now use dl nowledg 2 do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.4 A basic ml approach"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#start by densely connected network. following list shows a fully konectd model starting by flattening da data\n",
    "#and den runs it thru 2 Dense layers. lack of activation fn on the last Dense layer, typical 4 regression\n",
    "#problem. u can use MAE as da loss. coz u evaluate on the exact same data and with the exact same metric u did\n",
    "# with da common-sense approach the results will be directly comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.37 Training and evaluating a densely connected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model=Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.38 Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label= 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.5 A first recurrent baseline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gated recurrent unit (GRU) layers work using the same principle as LSTM, somewhat strealined and thus cheaper 2run.\n",
    "this trade-off btw computational expensiveness and representational power is seen everywhere in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.39 Training and evaluating a GRU-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(traing_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the new validation MAE of ~0.265 (before u start significantly overfitting) translates 2 a mae of 2.35 after\n",
    "denormalization. that's a solid gain on the initial error of 2.57 degrree c. still margin of improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.6 Using recurrent droputout to fight overfitting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "every recurrent layer in keras has two drop-out related args: droput, a float specifying the dropout rate 4 input\n",
    "units of the layer, and recurrent_dropout, specifying the dropout rate of the recurrent units. coz networks being\n",
    "regularized with dropout always take longer to fully converge, train da network 4 twice as many epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listin6.40 Training & evaluating a dropout-regularized GRU-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "no longer overfitting during the first 30 epochs. more stable eval scores & best scores not much lower dan b4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.7 Stacking recurrent layers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "no longer overfitting bt seem 2 have hit a performance bottleneck. consider increasing the network capacity.\n",
    "increase the no of units in the layer or add more layers .Recurrent layer stacking is a classic way 2 build more\n",
    "powerful recurrent networks. i.e. what currently powers the Google Translate algorithm is a stack of 7 large LSTM\n",
    "layers-that's huge.\n",
    "2 stack recurrent layers on top of each other in Keras all intermediate layers shud rtrn deir full seq of outputs\n",
    "(a 3D tensor) rather than their output at the last timestep. done by speciufying return_sequences = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.41 Training and evaluating a dropout-regularized, stacked GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5,))\n",
    "\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results show added layer does improve the results a bit not significantly. can draw 2 conclusions.\n",
    "\n",
    "1. coz still not overfitting 2 badly, u cud safely increase the size of ur layers in a quest 4 validation-loss\n",
    "improvement. this has a non-negligible computational cost, though.\n",
    "\n",
    "2. Adding a layer didnt help significantly, so diminishing returns 4rm increasing network capacity at dis point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.8 Using bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can offer greater performance than a regular RNN on certain tasks. called Swiss Army knife of dl 4 nlp. \n",
    "rnns notably order dependent or time dependent: dey process the timesteps of their input seqs in order, & \n",
    "shuffling or reversing da timesteps can completely change da representations the RNN extracts 4rm da sequence.\n",
    "a bidirectional RNN exploits the order sensitivity of RNNs: consists of using 2 regular RNNs such as the GRU\n",
    "and LSTM layers each of which processes the input seq in one order (chrnologically and antichronologically),\n",
    "and den merging their reps. by processing a seq both ways, a bidirectional RNN can catch patterns dat may be \n",
    "overlukd by a unidirectional RNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.42 Training & evaluating an LSTM using reversed seqs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "max_features = 10000       # No. of words 2 consider as features\n",
    "maxlen = 500               # cuts off texts after dis no. of words (among the max_features most common words)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test)  = imdb.load_data(num_words=max_features)    # loads data\n",
    "\n",
    "x_train =[x[::-1] for x in x_train]        # Reverses sequences\n",
    "x_test =[x[::-1] for x in x_test]                    \n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# To instantiate a bidirectional RNN in Keras, u use the Bidirectional layer, which takes as its\n",
    "1st arg a recurrent\n",
    "layer instance. Bidirectional creates a second, separate instance of this recurrent layer and uses one instance\n",
    "4 processing da input sequences in chronological order and the other instance for processing the input\n",
    "sequences in reversed order. let's try it on the IMDB sentiment-analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.43 Training and evaluating a bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics =['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "performs slightly better than regular LSTM achieving over 89% val_acc. also seems 2 overfit more quickly.\n",
    "unsurprising coz a bidirectional layer has twice as many params as a chronological LSTM. With some regularization,\n",
    "the bidirectional approach wud likely be a strong performer on this task.\n",
    "let's try the same approach on the temperature-prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.44 Training a bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(\n",
    "    layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "performs as well as regular GRU layer. all the predictive capacity must come from the chronological half ofda\n",
    "network, coz the antichronological half is known 2b severely underperforming on dis task(again coz the recent\n",
    "past matters much more than the distant past in this case.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.9 Going even further"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 improve performance on the temperature-forecasting problem.\n",
    "\n",
    "- adjust the no. of units in each recurrent layer in the stacked setup. The current choices are largely arbitrary\n",
    "  and thus probably suboptimal.\n",
    "\n",
    "- Adjust the learning rate used by the RMSprop optimizer.\n",
    "\n",
    "- Try using LSTM layers instead of GRU layers.\n",
    "\n",
    "- Try using a bigger densely connected regressor on top of the recurrent layers; i.e. a bigger Dense layer or even \n",
    "  a stack of Dense layers.\n",
    "    \n",
    "- Don't 4get 2 eventually run the best-performing models(in terms of validation MAE) on the  test set! Otherwise\n",
    "  u'll develop architectures that are overfitting to the validation set.\n",
    "    \n",
    "iterate repeatedly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.10 Wrapping Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when approaching a new problem, it's gud to establish common sense baselines 4 ur choice metric. if u dont have\n",
    "# a baseline 2 beat, u can't tell whether ur making real progress.\n",
    "\n",
    "# try simple models b4 expensive ones, to justify the additional expense. sometimes a simple modell turns best.\n",
    "\n",
    "#data wer temporal ordering matters, rnns a great fit & easily outperform models dat 1st flatten the temporal data.\n",
    "\n",
    "# droput in rnns, use time-constant dropout mask and recurrent dropout mask. built in Keras recurrent layers,\n",
    "# use the dropout and recurrent_dropout args of recurrent layers\n",
    "\n",
    "# Stacked rnns provide more representational power than a single RNN layer. dey r much more expensive and not always\n",
    "# worth it. although they offer clear gains on complex problems(such as machine translation), dey may not be \n",
    "# always be relevant to smaller, simpler problems.\n",
    "\n",
    "# Bidirectional RNNs, which luk at a sequence both ways are useful on nlp probs. not strong performers on sequence\n",
    "# data where the recent past is much more informative than the beginning of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Sequence processing with convnets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the same props that make convnets excel at cv also make them highly relevant to sequence processing. time can be\n",
    "treated as a spatial dimension, like the height or width of a 2D image.\n",
    "Such 1D convnets can be competitive with RNNs on certain sequence-processing problems usually at a considerably\n",
    "cheaper computational cost. Recently, 1D convnets, typically used with dilated kernels, have been used with great\n",
    "success for audio generation and machine translation. In addition to these specific successes, it has long been \n",
    "known that small 1D convnets can offer a fast alternative to RNNs for simple tasks such as text classification\n",
    "and timeseries forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.1 Understanding 1D convolution for sequence data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the conv layers introduced previously were 2D convolutions, extracting 2D patches 4rm image tensors and applying\n",
    "an identical transformation to every patch. in the same way, u can use 1D convolutions, extracting local 1D patches\n",
    "(subsequences) from sequences. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "such 1d conv layers can recognize local patterns in a sequence. coz the same input transformation is performed on\n",
    "every patch, a pattern learned at a a certain position in a sentence can later be recognized at a different \n",
    "position, making 1D convnets translation invariant (for temporal translations.) i.e. a 1D convnet processing \n",
    "sequences of characters using connvolution windows of size 5 shud be able to learn words or word fragments of \n",
    "length 5 or less, and it shud be able to recognize these words in any context in an input sequence.\n",
    "A character level 1D convnet is thus able  to learnt about word morpohology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.2 1D pooling for sequence data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2D pooling op has  1D equivlent: extracting 1D patches(subsequences) 4rm an input and outputting the max value\n",
    "(max pooling) or average value (average pooling). Just as with 2D convnets, this is used for reducing the length\n",
    "of 1D inputs(subsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.3 Implementing a 1D convnet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in keras, a 1D convnet is used via the Conv1D layer, an interface similar to Conv2D. takes an input 3D tensors\n",
    "with shape (samples, time, features) and returns similarly shaped 3D tensors. the convolution window is a 1D \n",
    "window on the temporal axis: axis 1 in the input tensor.\n",
    "let;s build a simple two-layer 1D convnet and apply it to the IMDB sentiment-classification task dat ur alredy\n",
    "familiar with. As a reminder, this is the code for obtaining and preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.45 Preparing the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "# u can use larger convolution windows with 1D convnets. With a 2D convolution layer, a 3 x 3 convolution window\n",
    "# contains 3 x 3 = 9 feature vectors; but with a 1D convolution layer; a conv window of size 3 contains only\n",
    "# 3 feature vectos. thus can easily afford 1D convolution windows of size 7 0r 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.4.6 Trainng and evaluating a simple 1D convnet on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label= 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label= 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_acc < dat of LSTM but runtime faster both on CPU and GPU. a convincing demo dat a 1D convnet can offer a fast\n",
    "cheap alternative 2 a rnn on a world-level sentiment clasification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.4 Combining CNNs and RNNs to process long sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "coz 1D convnets process input patches independently ,they arenst sensitive to the order of the timesteps(beyond a\n",
    "local scale, the size of the convolution windows), unlike RNNs. ofcourse, to recognize longer-term patterns u can \n",
    "stack many convolution layers and pooling layers resulting in upper layers dat will see long chunks of the \n",
    "original inputs-but that's still a fairly weak way to induce order sensitivity. One way 2 evidence this weakness\n",
    "is 2 try 1D convnets on the temperature-forecasting problem, where order-sensitivity is key to producing good\n",
    "predictions. the following e.g. reuses the dollowing variables defined previously: float_data, train_gen, val_gen\n",
    "and val_steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.47 Training & evaluating a simple 1D convnet on the Jena data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',\n",
    "                       input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label= 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_MAe stands in 0.40s. u cant even beat the common-sense baseline using the small convnet. coz convnet luks 4 \n",
    "patterns anywhere in the input timeseries and has no knowledge of the temporal position of a pattern it sees \n",
    "(toward the beginning, towards the end and so on). coz more recent data points shud be interpreted differently \n",
    "4rm older data points in the case of this specific forecasting problem, the convnet fails at producing \n",
    "meaningful results. this limitation of IMDB isnt an issue with the IMDB data, coz patterns of keywords \n",
    "associated with a +ve or -ve sentiment r informative independently of where they 're found in the input sentences.\n",
    "\n",
    "1 strategy 2 combine the speed and ligth og convnets with the order-sensitivity of RNNS is 2 use a 1D convnet \n",
    "as a preprocessing step b4 and RNN. esp benefecial wen dealing with v.long seqs as 1000s of steps. the convnet\n",
    "shall turn the long input seq in2 much shorter (downsampled) seqs of higher-level features. dis seq of extracted\n",
    "features den becomes the input to the RNN part of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.48 Preparing higher-resolution data generators for the Jena dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step = 3                           //prev set to 6(1 pt /hr) now 3 (1 pt/0.5hr)\n",
    "lookback = 720\n",
    "delay = 144\n",
    "\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step)\n",
    "val_gen  =  generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=200001,\n",
    "                      max_index=300000,\n",
    "                      step=step)\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=300001,\n",
    "                      max_index=None,\n",
    "                      step=step)\n",
    "\n",
    "val_steps = (300000 - 200001 - lookback) // 128\n",
    "# How many steps 2 draw 4rm val_gen in order 2 c da entire validation set\n",
    "\n",
    "test_steps = (len(float_data) - 300001 - lookback) // 128\n",
    "# steps 2draw 4rm test_gen in order2 c da entire test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6.49 Model combining a 1D convolutional base & a GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',\n",
    "                       input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label= 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "judging 4rm val_loss this setup isnt a gud as the regularized GRU alone, but it's significantly faster. it luks\n",
    "at twice as much data, which in this case doesnt appear 2b hugely helpful but may be imp 4 other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.5 Wrapping Up"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in the same way dat 2D convnets performs well 4 processing visual patterns in 2D space, 1D convnets perform well\n",
    "4 processing temporal patterns. they offer a faster alternative 2 RNNs on somr problems, in particular nlp tasks.\n",
    "\n",
    "typically, 1d convnets  are structured much like their 2D eqs 4rm cv world, dey consist of Conv1D layers and \n",
    "Max-Pooling1d layers ending in a global pooling op or flattening op.\n",
    "\n",
    "coz RNNs r extremely expensive 4 processing vry long seqs, but 1D convnets r cheap, it can be a gud idea to use a \n",
    "1D convnet as a preprocessing step b4 an RNN, shortening the sequence and extracting useful reps 4da RNN 2 process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter Summary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learnt\n",
    "-how to tokenize text\n",
    "-what word embeddings are and how to use them\n",
    "- what recurrent networks are and how to use them\n",
    "-How to tack RNN layers and use bidirectional RNNs to build more powerful sequence processing models\n",
    "-How to use 1D convnets for sequence processing\n",
    "-How to combine 1D convnets and RNNs to process long sequences.\n",
    "\n",
    "RNNs can be used for timeseries regression(\"predicting the future\"), timeseries clalssifciation, anomaly detection \n",
    "in timeseries, and sequence labelling (such as identifying names or dates in sentences).\n",
    "\n",
    "similary 1D convnets can be used for machine translation( sequence to sequence convolutional models lie SLiceNet),\n",
    "document classification and spelling correction\n",
    "\n",
    "if global order matters in your seq data then its preferable to use a rnn 2 process it. typically 4 timeseries, \n",
    "where recent past is more informative dan distant past.\n",
    "\n",
    "if global ordering isnt fundamentally meaningful, then 1D convnets will turn out to work atleast as well and \n",
    "are cheaper. this is often the case for text datam where a keyword found at the beginning of a sentence is just \n",
    "as meaningful as a keyword found at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
