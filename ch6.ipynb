{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning  For Text and Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dl models processing text(seq of words or characters), timeseries or seq of data in general. 2 fund dl algos r recurrent neural\n",
    "networks and ID convnets. \n",
    "Apps:-\n",
    "    document classification and timeseries classification, such as identifying the topi or author\n",
    "    timeseries comparisons such as estimating how closely related two documents or two stock tickers are\n",
    "    se1 2 seq learning such as deconding an English sentence into French\n",
    "    Sentiment analysis such as classifying the sentiment of tweets or movie reviews as +ve or -ve\n",
    "    Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data\n",
    "    \n",
    "E.gs:-\n",
    "    sentiment analysis on the IMDB dataset\n",
    "    temperature forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Working with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dl 4 nlp is patten recognition applied 2 words, sentences & paragraphs in much the same way dat omputer vision is pattern\n",
    "recognition applied 2 pixels. numeric tensors fed into nns as input rather than raw text. Vectorizing text is da process\n",
    "of transforming text in2 numeric tensors. this can b done in multiple ways:-\n",
    "    segment text into words, and transform each word into a vector\n",
    "    segment text into characters and transform each character into a vector\n",
    "    extract n-grams of words or characters, and transform each n-gram into a vector. N-grams r overlapping groups of multiple\n",
    "    consecutive groups or characters.\n",
    "\n",
    "Collectively, the different units in which u can break down text(words, character, or n-grams) are called tokens, and breaking text in2 such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme\n",
    "and then associating numeric vectors with the generated tokens. These vectors, packed into sequence tokens are fed in2 deep nns.There r multiple ways 2 associate a vector with a token. In this section, I will present 2 major ones: one-hot encoding of\n",
    "tokens and token-embedding(usd excl 4 words kald word embedding). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding n-grams & bag of words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word n-grams r grups of N (or fewer) consecutive words that u can extract 4rm a sentence. The same concept may be applied to \n",
    "characters instead of words. \n",
    "\n",
    "Consider da example \" The cat sat on the mat\"  may be decomposed in2 the foloowing set of 2-grams:-\n",
    "    {\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n",
    "    \n",
    "    may also be decomposed into following set of 3-grams:-\n",
    "        {\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\", \"sat on the\",\n",
    "        \"the mat\", \"mat\", \"on the mat\"}\n",
    "        \n",
    "Such above set is called bag-of-2 grams or bad-of-3-grams respectively. The term bag refers 2da fact that ur dealing with a set \n",
    "of tokens rather than a list or sequence: the tokens have no specific order. This family of tokenization is kald bag-of-words.\n",
    "    \n",
    "1d cns and rnns r capable of learning reps 4 groups of wordsncharacters vidout being explicitly told abt da existence of such\n",
    "groups, by luking at continuous word or character sequences. however dey r unavoidable feature-engineering tools when using lightweight,\n",
    "shallow text-processing models such as logistic regression and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.1 One hot-encding of words and characters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "most comon way 2 convert token in2 vector. associates a unique integer index with every word and then turns that integer index i\n",
    "into a binary vector of size N (the size of vocabulary); the vector is all zeros except for the ith entry, which is 1. \n",
    "e.g. 6.1 4 words\n",
    "e.g. 6.2 4 characers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Word-level one-hot encoding(toy-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.' ] # initial data: one entry/sample(here a sample is a sentence\n",
    "                                                                   # bit it cud b an entire document)\n",
    "    \n",
    "token_index = {}     #Builds an index of all tokens in the data\n",
    "for sample in samples:         \n",
    "    for word in sample.split():  # Tokenizes da sample via split method. In real life, strip punctuaton & special characters.\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1  #assigns uniq index 2each uniq word. index 0 nt attributed 2 anything.\n",
    "            \n",
    "    max_length = 10    # vectorizes the samples. only consider da 1st max_length words in each sample.\n",
    "    \n",
    "    results = np.zeros(shape=(len(samples),\n",
    "                              max_length,\n",
    "                              max(token_index.values()) + 1))   # results r stored here\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "            index = token_index.get(word)\n",
    "            results[i, j, index] = 1\n",
    "            \n",
    "\n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 1,\n",
       " 'cat': 2,\n",
       " 'sat': 3,\n",
       " 'on': 4,\n",
       " 'the': 5,\n",
       " 'mat.': 6,\n",
       " 'dog': 7,\n",
       " 'ate': 8,\n",
       " 'my': 9,\n",
       " 'homework.': 10}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Character-level one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable   # All printable ASCII charcters\n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras has buitlin utilites 4 one-hot encoding of text @ word/character levels starting 4rm raw data. should use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Using Keras for word-level one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)     #Creates a tokenizer, configd to take only into account the 1000 most common words\n",
    "tokenizer.fit_on_texts(samples)         # Builds the word index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)    #Turns strings into lists of integer indices\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')  #Cud also directly get 1-hot binary reps.\n",
    "                        # Vectorization modes other than one-hot encoding are supported b y this tokenizer\n",
    "    \n",
    "word_index = tokenizer.word_index     # How u can recover the word index that was computed\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot hashing trick"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "used wen no. of unique tokens is vocublary is 2 lrg 2b handled explicitly. Instead of explicitly assigning an index 2 each word\n",
    "and keeping a reference of these indices in a dictionary, u can hash words into vectors of fixed size. This is typically donw\n",
    "with a very ligthweight hashking function. The main advantage of this method is that it does away with maintaining an eplicit word\n",
    "index, which saves memory and allows online encoding of the data( you can generate token vectors right away, b4 u've seen all\n",
    "of the data). Drawbaq=> susceptible 2 hash collissions i.e. 2 different words may end up with the same hash., and subsquently\n",
    "any ml model luking at these hhashes wont beable to tell the diff btw these words. The likelihood of hash collissions decreses\n",
    "when the dimensionality of the hashing space is much larger than the total no. of uniquetokens beng hashed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Word-Level one-hot encoding with hashing trick (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework' ]\n",
    "\n",
    "dimensionality = 1000    # Stores the words as vectors of size 1000. If u've close 2 1000 words(or more), u'll see many hash \n",
    "                         # collissions, which will decrease the accuracy of this encoding method.\n",
    "max_length= 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality  # Hashes the word into a random index integer btw 0 and 1000\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.2 Using word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "low-dimensional floating-point vectors(dense vectors as opposed to sparse vectors). Unlike the word vectors obtained via one-hot\n",
    "encoding, word embeddings are learned from data. it's common 2c word-embeddings dat r 256-dimensional, 512-dimsensional, 1024\n",
    "dimensional when dealing with very lrg vocabularies. On the other hand, one-hot encodings words generally leads to vectors that\n",
    "r 20,000 dimensional or greatere(capturing a vocabularty of 20000 tokens in this case). so word embeddingsz pack more info into \n",
    "far fewer dimensions/.\n",
    "\n",
    "one-hot word vectors are sparse-highdimensional and hardcored.\n",
    "word embeddings are dense, lower-dimensional and learned from data.\n",
    "\n",
    "2 ways to obtain word embeddings\n",
    "\n",
    "1- learn woprd embeddings hointly with main task u care abt. (such as document classification or sentiment prediction). u start \n",
    "with random vectors and then learn word vectors in the same way u learn the weights of a nn.\n",
    "\n",
    "2. load into your model word embeddings that were precomputed using a different machine-learning task than the one u'r trying \n",
    "to solve. These are called pretrained word embeddings.\n",
    "\n",
    "let's luk at both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning word embeddings with the embedding layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "simplest way 2 associate a dense vector with a word is2 choose the vector at random. The problem with ths approach is that the \n",
    "resulting embedding space has no structure: i.e., the words accurate and exact may end up wth completely different \n",
    "embeddings, even though they r interchangable in most sentences. its difficult for a dnn 2 make sense of such a noisy, unstructured embedding \n",
    "space.\n",
    "2get a bit more abstract, the geometric relationships btw word vectors should reflect the semantic relationships btw these \n",
    "words. word embeddings r meant to map human language into a geometric space. i.e. in a reasonable embedding space, u shud expect \n",
    "synonyms 2b embedded into similar word vectors; and in general u wud expect the geometric distance (such as L2 distance) btw any\n",
    "2 word vectors 2 relate to the semantic distance bt2 the associiated words(words meaning diff things r embedded at points far \n",
    "away 4rm each other, whereas relate words are closer.). In addition to distance, u many want specific directions in the embedding\n",
    "space to be meaninful.\n",
    "e.g., embed 4 words on a 2D plane: cat, dog, wold and tiger. With the vector reps we chose jhere, some semantic relationships\n",
    "btw these words can b encoded as geometric transformations. i.e. \"pet to wild animal\" vector allows us2 go 4rm cat 2 tiger & 4rm\n",
    "dog 2 wolf. \"from canine to feline\" vector allows us2 go 4rm dog 2 cat and wold 2 tiger.\n",
    "real world word embedding spaces, eg';s of meaningful geometric transformations are \"gender\" vectors and \"plural\" vectors. i.e.\n",
    "by addn a femal vector 2 da vector king we get vector queen and addn plural vectoer we obtain kings. word embedding spaces \n",
    "typically feature thousands of such interpretable and potentially useful vectors.\n",
    "its thus reasonable to learn a new embedding space with every new task. backpropagation makes this easy and Keras makes it even easier. Its \n",
    "abt learning the weights of a layer: the Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Instantiating an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000, 64)  #The Embedding Layer takes at least 2 args: the no. of possible tokens(here, 1,000\n",
    "                                       # 1 + maximum word index) and the dimensionality of the embeddings(here, 64)\n",
    "    \n",
    "#The embedding layer is best understodd as a dictionary that maps integer indices (which stand for specific words) 2 dense vectors.\n",
    "#It takes integers as input, it luks up these integers in an internal dictionary, and it returns the associated vectors. it's\n",
    "#effectively a dictionary lookup.\n",
    "\n",
    "# word index => Embedding layer => Corresponding word vector\n",
    "\n",
    "#the embedding layer takes as input a 2D tensor of integers ,of shape( samples, sequence_length), where each entry is a seq of\n",
    "# integers. it can embed sequences of variable lengths: i.e. u cud feed in2 da Embedding Layer in the previous example batches\n",
    "# with shapes (32, 10)(batch of 32 sequences of length 10) or (64, 15)(batch of 64 sequences of length 15). All seqs in a \n",
    "# batch mst have the same length, though(as they have 2b packed in2 a single tensor), so seqs shorter than others shud b\n",
    "# padded with zeros, and seqs dat r longer shud be truncated.\n",
    "\n",
    "# dis layer return a 3D floating point tensor of shape (samples, sequence_length, embedding_dimensionality). Such a 3D tensor\n",
    "# can then b processed by an RNN layer or a 1D convolution layer. \n",
    "\n",
    "# wen u instantiate an EMbedding latyer, its weights (its internal dict of token vectors) are initally random, just as with \n",
    "# any other layer. Dring training, these word vectors are gradually adjusted via backpropagation, structuring the space in2 sth\n",
    "# the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure- a kind of structure\n",
    "# specialized for the specific problem 4 which ur training ur model.\n",
    "\n",
    "# let's apply his idea to the IMDB movie-review sentiment prediction task. 1st. prepare data. restrict move review 2 top 10000\n",
    "# mst common words and cut off the review after only 20 words. THe network will learn 8 dimensiaol embdngs 4 each of the 10000\n",
    "# words, turn the input integer sequences (2D integer tensor) in2 embedded sequences (3D float tensor), flatten the tensor 2\n",
    "# 2D, and train a single Dense layer on top 4 classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6 Loading the IMDB data for use with an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000   # no. of words 2 consider as features\n",
    "maxlen = 20     #  cuts off the text after this no. of words(among the max_features most common words)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)    # loads da data as list of integerz\n",
    "\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)   # turns the list of integers in2 a 2D integer tensor \n",
    "                                                                         # of shape (samples, maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.7 Using an  Embedding Layer and classifier on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 3s 2ms/step - loss: 0.6863 - acc: 0.5657 - val_loss: 0.6218 - val_acc: 0.6982\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.5695 - acc: 0.7444 - val_loss: 0.5229 - val_acc: 0.7312\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.4684 - acc: 0.7834 - val_loss: 0.4996 - val_acc: 0.7438\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.4281 - acc: 0.8046 - val_loss: 0.4931 - val_acc: 0.7524\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3981 - acc: 0.8209 - val_loss: 0.4940 - val_acc: 0.7550\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3750 - acc: 0.8349 - val_loss: 0.4994 - val_acc: 0.7558\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3600 - acc: 0.8414 - val_loss: 0.5029 - val_acc: 0.7570\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3318 - acc: 0.8604 - val_loss: 0.5074 - val_acc: 0.7574\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3116 - acc: 0.8712 - val_loss: 0.5139 - val_acc: 0.7564\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2997 - acc: 0.8784 - val_loss: 0.5230 - val_acc: 0.7524\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))  #Specifies da max input length 2da Embedding layer so u can later flatten\n",
    "                        # da embedded inputs. Aftr da Embedding layer, the activations have shape (samples, maxlen, 8)\n",
    "\n",
    "model.add(Flatten())    # Flattens the 3D tensor of embeddings in2 a 2D tensor of shape(samples, maxlen * 8)\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))       # adds the classifier on top\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val accuracy of 75%. better add recurrnt layers or !D con layers ontop ofda embedded seqs to take into accnt each seq asawhole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#wen little data available, load embedding vectors 4rm a precomputed embedding space which is highly structured and exhibits\n",
    "# useful props-capturing gen aspects of lang struct. \n",
    "# such word embeddings r generally computed using word-occirence statistics (observations abt what words co-occur in \n",
    "# sentences or documents), using a variety of techniques, some involving nns others not. \n",
    "# der r various precomputed dbs of wordembeddings dat can be downloaded nd used in Keras Embedding Layer. \n",
    "#Word2vec\n",
    "#GloVe(Global Vectors for World Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: 4rm raw text to word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model=embedding sentences into a seq of vectors => flatetning tehm => training a dense layer on top    ===using \n",
    "   pretrained word embeddings  ////strtng 4rm scratch by odownloading the orgnal text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the IMDB data as raw text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://mng.bz/0tlo.       download the raw IMDB dataset. uncompress it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.8 Processing the labels of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/Omar/Downloads/aclImdb/aclImdb/test/train\\\\neg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-4b8940b4d027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'neg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdir_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/Omar/Downloads/aclImdb/aclImdb/test/train\\\\neg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = 'C:/Users/Omar/Downloads/aclImdb/aclImdb/test/'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    " \n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
